{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13770262,"sourceType":"datasetVersion","datasetId":8761833},{"sourceId":13774526,"sourceType":"datasetVersion","datasetId":8767069},{"sourceId":13776162,"sourceType":"datasetVersion","datasetId":8768305}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importations","metadata":{}},{"cell_type":"code","source":"import nltk\nimport pandas as pd\nimport chardet\nfrom nltk.tokenize import word_tokenize\nfrom xgboost import XGBClassifier\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud\nfrom nltk import pos_tag\nimport pickle\nimport os\nimport string\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import TruncatedSVD\nimport re\nimport numpy as np\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:08:35.967411Z","iopub.execute_input":"2025-11-20T15:08:35.968183Z","iopub.status.idle":"2025-11-20T15:08:38.656315Z","shell.execute_reply.started":"2025-11-20T15:08:35.968148Z","shell.execute_reply":"2025-11-20T15:08:38.655326Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"nltk.download(\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:08:38.657797Z","iopub.execute_input":"2025-11-20T15:08:38.658334Z","iopub.status.idle":"2025-11-20T15:09:00.827336Z","shell.execute_reply.started":"2025-11-20T15:08:38.658301Z","shell.execute_reply":"2025-11-20T15:09:00.826369Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading collection 'all'\n[nltk_data]    | \n[nltk_data]    | Downloading package abc to /usr/share/nltk_data...\n[nltk_data]    |   Package abc is already up-to-date!\n[nltk_data]    | Downloading package alpino to /usr/share/nltk_data...\n[nltk_data]    |   Package alpino is already up-to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping\n[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping\n[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping\n[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n[nltk_data]    | Downloading package basque_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package basque_grammars is already up-to-date!\n[nltk_data]    | Downloading package bcp47 to /usr/share/nltk_data...\n[nltk_data]    | Downloading package biocreative_ppi to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n[nltk_data]    | Downloading package bllip_wsj_no_aux to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n[nltk_data]    | Downloading package book_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package book_grammars is already up-to-date!\n[nltk_data]    | Downloading package brown to /usr/share/nltk_data...\n[nltk_data]    |   Package brown is already up-to-date!\n[nltk_data]    | Downloading package brown_tei to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package brown_tei is already up-to-date!\n[nltk_data]    | Downloading package cess_cat to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cess_cat is already up-to-date!\n[nltk_data]    | Downloading package cess_esp to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cess_esp is already up-to-date!\n[nltk_data]    | Downloading package chat80 to /usr/share/nltk_data...\n[nltk_data]    |   Package chat80 is already up-to-date!\n[nltk_data]    | Downloading package city_database to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package city_database is already up-to-date!\n[nltk_data]    | Downloading package cmudict to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cmudict is already up-to-date!\n[nltk_data]    | Downloading package comparative_sentences to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n[nltk_data]    | Downloading package comtrans to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package comtrans is already up-to-date!\n[nltk_data]    | Downloading package conll2000 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package conll2000 is already up-to-date!\n[nltk_data]    | Downloading package conll2002 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package conll2002 is already up-to-date!\n[nltk_data]    | Downloading package conll2007 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package conll2007 is already up-to-date!\n[nltk_data]    | Downloading package crubadan to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package crubadan is already up-to-date!\n[nltk_data]    | Downloading package dependency_treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package dependency_treebank is already up-to-date!\n[nltk_data]    | Downloading package dolch to /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/dolch.zip.\n[nltk_data]    | Downloading package english_wordnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/english_wordnet.zip.\n[nltk_data]    | Downloading package europarl_raw to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package europarl_raw is already up-to-date!\n[nltk_data]    | Downloading package extended_omw to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package floresta to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package floresta is already up-to-date!\n[nltk_data]    | Downloading package framenet_v15 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n[nltk_data]    | Downloading package framenet_v17 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n[nltk_data]    | Downloading package gazetteers to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package gazetteers is already up-to-date!\n[nltk_data]    | Downloading package genesis to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package genesis is already up-to-date!\n[nltk_data]    | Downloading package gutenberg to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package gutenberg is already up-to-date!\n[nltk_data]    | Downloading package ieer to /usr/share/nltk_data...\n[nltk_data]    |   Package ieer is already up-to-date!\n[nltk_data]    | Downloading package inaugural to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package inaugural is already up-to-date!\n[nltk_data]    | Downloading package indian to /usr/share/nltk_data...\n[nltk_data]    |   Package indian is already up-to-date!\n[nltk_data]    | Downloading package jeita to /usr/share/nltk_data...\n[nltk_data]    |   Package jeita is already up-to-date!\n[nltk_data]    | Downloading package kimmo to /usr/share/nltk_data...\n[nltk_data]    |   Package kimmo is already up-to-date!\n[nltk_data]    | Downloading package knbc to /usr/share/nltk_data...\n[nltk_data]    |   Package knbc is already up-to-date!\n[nltk_data]    | Downloading package large_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package large_grammars is already up-to-date!\n[nltk_data]    | Downloading package lin_thesaurus to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n[nltk_data]    | Downloading package mac_morpho to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package mac_morpho is already up-to-date!\n[nltk_data]    | Downloading package machado to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package machado is already up-to-date!\n[nltk_data]    | Downloading package masc_tagged to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package masc_tagged is already up-to-date!\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping\n[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n[nltk_data]    | Downloading package mock_corpus to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/mock_corpus.zip.\n[nltk_data]    | Downloading package moses_sample to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package moses_sample is already up-to-date!\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package movie_reviews is already up-to-date!\n[nltk_data]    | Downloading package mte_teip5 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package mte_teip5 is already up-to-date!\n[nltk_data]    | Downloading package mwa_ppdb to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n[nltk_data]    | Downloading package names to /usr/share/nltk_data...\n[nltk_data]    |   Package names is already up-to-date!\n[nltk_data]    | Downloading package nombank.1.0 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package nonbreaking_prefixes to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n[nltk_data]    | Downloading package nps_chat to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package nps_chat is already up-to-date!\n[nltk_data]    | Downloading package omw to /usr/share/nltk_data...\n[nltk_data]    |   Package omw is already up-to-date!\n[nltk_data]    | Downloading package omw-1.4 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package opinion_lexicon to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n[nltk_data]    | Downloading package panlex_swadesh to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package paradigms to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package paradigms is already up-to-date!\n[nltk_data]    | Downloading package pe08 to /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/pe08.zip.\n[nltk_data]    | Downloading package perluniprops to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping misc/perluniprops.zip.\n[nltk_data]    | Downloading package pil to /usr/share/nltk_data...\n[nltk_data]    |   Package pil is already up-to-date!\n[nltk_data]    | Downloading package pl196x to /usr/share/nltk_data...\n[nltk_data]    |   Package pl196x is already up-to-date!\n[nltk_data]    | Downloading package porter_test to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package porter_test is already up-to-date!\n[nltk_data]    | Downloading package ppattach to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package ppattach is already up-to-date!\n[nltk_data]    | Downloading package problem_reports to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package problem_reports is already up-to-date!\n[nltk_data]    | Downloading package product_reviews_1 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n[nltk_data]    | Downloading package product_reviews_2 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n[nltk_data]    | Downloading package propbank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package propbank is already up-to-date!\n[nltk_data]    | Downloading package pros_cons to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package pros_cons is already up-to-date!\n[nltk_data]    | Downloading package ptb to /usr/share/nltk_data...\n[nltk_data]    |   Package ptb is already up-to-date!\n[nltk_data]    | Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]    |   Package punkt is already up-to-date!\n[nltk_data]    | Downloading package punkt_tab to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package punkt_tab is already up-to-date!\n[nltk_data]    | Downloading package qc to /usr/share/nltk_data...\n[nltk_data]    |   Package qc is already up-to-date!\n[nltk_data]    | Downloading package reuters to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package reuters is already up-to-date!\n[nltk_data]    | Downloading package rslp to /usr/share/nltk_data...\n[nltk_data]    |   Package rslp is already up-to-date!\n[nltk_data]    | Downloading package rte to /usr/share/nltk_data...\n[nltk_data]    |   Package rte is already up-to-date!\n[nltk_data]    | Downloading package sample_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sample_grammars is already up-to-date!\n[nltk_data]    | Downloading package semcor to /usr/share/nltk_data...\n[nltk_data]    |   Package semcor is already up-to-date!\n[nltk_data]    | Downloading package senseval to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package senseval is already up-to-date!\n[nltk_data]    | Downloading package sentence_polarity to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sentence_polarity is already up-to-date!\n[nltk_data]    | Downloading package sentiwordnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sentiwordnet is already up-to-date!\n[nltk_data]    | Downloading package shakespeare to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package shakespeare is already up-to-date!\n[nltk_data]    | Downloading package sinica_treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sinica_treebank is already up-to-date!\n[nltk_data]    | Downloading package smultron to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package smultron is already up-to-date!\n[nltk_data]    | Downloading package snowball_data to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package snowball_data is already up-to-date!\n[nltk_data]    | Downloading package spanish_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package spanish_grammars is already up-to-date!\n[nltk_data]    | Downloading package state_union to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package state_union is already up-to-date!\n[nltk_data]    | Downloading package stopwords to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package stopwords is already up-to-date!\n[nltk_data]    | Downloading package subjectivity to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package subjectivity is already up-to-date!\n[nltk_data]    | Downloading package swadesh to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package swadesh is already up-to-date!\n[nltk_data]    | Downloading package switchboard to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package switchboard is already up-to-date!\n[nltk_data]    | Downloading package tagsets to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package tagsets is already up-to-date!\n[nltk_data]    | Downloading package tagsets_json to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping help/tagsets_json.zip.\n[nltk_data]    | Downloading package timit to /usr/share/nltk_data...\n[nltk_data]    |   Package timit is already up-to-date!\n[nltk_data]    | Downloading package toolbox to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package toolbox is already up-to-date!\n[nltk_data]    | Downloading package treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package treebank is already up-to-date!\n[nltk_data]    | Downloading package twitter_samples to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package twitter_samples is already up-to-date!\n[nltk_data]    | Downloading package udhr to /usr/share/nltk_data...\n[nltk_data]    |   Package udhr is already up-to-date!\n[nltk_data]    | Downloading package udhr2 to /usr/share/nltk_data...\n[nltk_data]    |   Package udhr2 is already up-to-date!\n[nltk_data]    | Downloading package unicode_samples to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package unicode_samples is already up-to-date!\n[nltk_data]    | Downloading package universal_tagset to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package universal_tagset is already up-to-date!\n[nltk_data]    | Downloading package universal_treebanks_v20 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n[nltk_data]    |       date!\n[nltk_data]    | Downloading package vader_lexicon to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package vader_lexicon is already up-to-date!\n[nltk_data]    | Downloading package verbnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package verbnet is already up-to-date!\n[nltk_data]    | Downloading package verbnet3 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n[nltk_data]    | Downloading package webtext to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package webtext is already up-to-date!\n[nltk_data]    | Downloading package wmt15_eval to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n[nltk_data]    | Downloading package word2vec_sample to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package word2vec_sample is already up-to-date!\n[nltk_data]    | Downloading package wordnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wordnet is already up-to-date!\n[nltk_data]    | Downloading package wordnet2021 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package wordnet2022 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n[nltk_data]    | Downloading package wordnet31 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package wordnet_ic to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wordnet_ic is already up-to-date!\n[nltk_data]    | Downloading package words to /usr/share/nltk_data...\n[nltk_data]    |   Package words is already up-to-date!\n[nltk_data]    | Downloading package ycoe to /usr/share/nltk_data...\n[nltk_data]    |   Package ycoe is already up-to-date!\n[nltk_data]    | \n[nltk_data]  Done downloading collection all\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# Functions","metadata":{}},{"cell_type":"code","source":"def summary(dataframe):\n    print(\"\\t Head\")\n    print(dataframe.head())\n    print(\"\\t Description\")\n    print(dataframe.describe())\n\ndef null_checker(dataframe):\n    print(dataframe.isnull().sum())\n\ndef duplicated_checker(dataframe):\n    print(dataframe.loc[dataframe.duplicated(),:])\n\n\ndef reader(path):\n    with open(path, 'rb') as f:\n        result = chardet.detect(f.read())\n    reviews = pd.read_csv(path, encoding=result['encoding'])    \n    return reviews    \n\ndef tokenizer(dataset):\n    new_dataset = dataset.copy(deep=True)\n    new_dataset[\"review\"] = new_dataset.review.map(lambda x: word_tokenize(x.lower() if isinstance(x, str) else str(x) ))\n    return new_dataset\n\ndef stopwords_remover(dataset):    \n    with open(\"/kaggle/input/imdb-reviews/stopwords.txt\") as file:\n        custom_stopwords = file.read().split(\",\")\n        \n    stop_words = set(stopwords.words('english') + custom_stopwords + [\"footnote\", \"sidenote\", \"project\", \"gutenberg\"])\n\n    regex = r\"^\\w+$\"\n\n    dataset[\"review\"] = dataset.review.map(lambda x: [word for word in x if (word not in stop_words and re.match(regex, word))])\n    return dataset\n\ndef punctuation_remover(dataset):\n    punctuation = string.punctuation + \"``\" + \"''\" + \"--\" + \"_\" + \"(\" + \")\" + '\"\"' + \"|\" + \"“\" + \"”\" + \"’\" + \"‘\" + \"___\"\n    dataset[\"review\"] = dataset.review.map(lambda x: [word for word in x if word not in punctuation])\n    return dataset\n\ndef pos_tagger(dataset):\n    dataset[\"review\"] = dataset.review.map(lambda x: [tagged for tagged in pos_tag(x,tagset='universal') if tagged[1] not in [\"NUM\"] ])\n    return dataset\n\ndef lemmatizer(dataset):\n    lem = WordNetLemmatizer()\n    dataset[\"review\"] = dataset.review.map(lambda row: [ lem.lemmatize(word[0], pos = get_pos_tag(word[1])) for word in row ])\n    return dataset    \n\ndef get_pos_tag(pos):    \n    match pos:\n        case \"NOUN\":\n            result = \"n\"\n        case \"VERB\":\n            result = \"v\"\n        case \"ADJ\":\n            result = \"a\"\n        case \"ADV\":\n            result = \"r\"\n        case _:\n            result = \"s\"\n    return result\n\ndef tf_idf_calculator(title, content):\n\n    vectorizer = TfidfVectorizer(strip_accents = \"ascii\", max_df = 0.6)\n    m = vectorizer.fit_transform(content).transpose() \n       \n    return  (pd.DataFrame( data = m.toarray(), index = vectorizer.vocabulary_, columns = title), vectorizer)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2025-11-20T15:09:00.828271Z","iopub.execute_input":"2025-11-20T15:09:00.828542Z","iopub.status.idle":"2025-11-20T15:09:00.843445Z","shell.execute_reply.started":"2025-11-20T15:09:00.828521Z","shell.execute_reply":"2025-11-20T15:09:00.842387Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"try:\n    dataset = reader(\"/kaggle/input/imdb-reviews/imdb/imdb.csv\")\nexcept Exception as e:\n    print(f\"Something when wrong while reading the file: \\n{e}\")","metadata":{"execution":{"iopub.status.busy":"2025-11-20T15:09:00.845599Z","iopub.execute_input":"2025-11-20T15:09:00.845874Z","iopub.status.idle":"2025-11-20T15:14:56.117651Z","shell.execute_reply.started":"2025-11-20T15:09:00.845856Z","shell.execute_reply":"2025-11-20T15:14:56.116556Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"dataset[dataset.duplicated()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:14:56.118776Z","iopub.execute_input":"2025-11-20T15:14:56.119116Z","iopub.status.idle":"2025-11-20T15:14:56.565592Z","shell.execute_reply.started":"2025-11-20T15:14:56.119087Z","shell.execute_reply":"2025-11-20T15:14:56.564780Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [Unnamed: 0, type, review, label, file]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>type</th>\n      <th>review</th>\n      <th>label</th>\n      <th>file</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"null_checker(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:14:56.566418Z","iopub.execute_input":"2025-11-20T15:14:56.566674Z","iopub.status.idle":"2025-11-20T15:14:56.605284Z","shell.execute_reply.started":"2025-11-20T15:14:56.566621Z","shell.execute_reply":"2025-11-20T15:14:56.604224Z"}},"outputs":[{"name":"stdout","text":"Unnamed: 0    0\ntype          0\nreview        0\nlabel         0\nfile          0\ndtype: int64\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"dataset.label.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:14:56.606385Z","iopub.execute_input":"2025-11-20T15:14:56.606727Z","iopub.status.idle":"2025-11-20T15:14:56.632517Z","shell.execute_reply.started":"2025-11-20T15:14:56.606699Z","shell.execute_reply":"2025-11-20T15:14:56.631566Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"label\nunsup    50000\nneg      25000\npos      25000\nName: count, dtype: int64"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"dataset.drop([\"Unnamed: 0\",\"file\"], axis=1, inplace=True)\ndataset.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:14:56.633652Z","iopub.execute_input":"2025-11-20T15:14:56.633992Z","iopub.status.idle":"2025-11-20T15:14:56.665950Z","shell.execute_reply.started":"2025-11-20T15:14:56.633961Z","shell.execute_reply":"2025-11-20T15:14:56.665008Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   type                                             review label\n0  test  Once again Mr. Costner has dragged out a movie...   neg\n1  test  This is an example of why the majority of acti...   neg\n2  test  First of all I hate those moronic rappers, who...   neg\n3  test  Not even the Beatles could write songs everyon...   neg\n4  test  Brass pictures (movies is not a fitting word f...   neg","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>review</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>test</td>\n      <td>Once again Mr. Costner has dragged out a movie...</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>test</td>\n      <td>This is an example of why the majority of acti...</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>test</td>\n      <td>First of all I hate those moronic rappers, who...</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>test</td>\n      <td>Not even the Beatles could write songs everyon...</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>test</td>\n      <td>Brass pictures (movies is not a fitting word f...</td>\n      <td>neg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"rows_to_drop = dataset[dataset[\"label\"] == 2].index\ndataset.drop(rows_to_drop, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:14:56.666962Z","iopub.execute_input":"2025-11-20T15:14:56.667335Z","iopub.status.idle":"2025-11-20T15:14:56.691600Z","shell.execute_reply.started":"2025-11-20T15:14:56.667303Z","shell.execute_reply":"2025-11-20T15:14:56.690724Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"dataset[\"label\"] = dataset.label.apply(lambda label: 0 if label == \"neg\" else 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:14:56.694972Z","iopub.execute_input":"2025-11-20T15:14:56.695276Z","iopub.status.idle":"2025-11-20T15:14:56.737797Z","shell.execute_reply.started":"2025-11-20T15:14:56.695254Z","shell.execute_reply":"2025-11-20T15:14:56.736689Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"tokenized_reviews = tokenizer(dataset)\ntokenized_reviews","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:14:56.738755Z","iopub.execute_input":"2025-11-20T15:14:56.739017Z","iopub.status.idle":"2025-11-20T15:17:24.873998Z","shell.execute_reply.started":"2025-11-20T15:14:56.738997Z","shell.execute_reply":"2025-11-20T15:17:24.873004Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"        type                                             review  label\n0       test  [once, again, mr., costner, has, dragged, out,...      0\n1       test  [this, is, an, example, of, why, the, majority...      0\n2       test  [first, of, all, i, hate, those, moronic, rapp...      0\n3       test  [not, even, the, beatles, could, write, songs,...      0\n4       test  [brass, pictures, (, movies, is, not, a, fitti...      0\n...      ...                                                ...    ...\n99995  train  [delightfully, awful, !, made, by, david, gian...      1\n99996  train  [watching, time, chasers, ,, it, obvious, that...      1\n99997  train  [at, the, beginning, we, can, see, members, of...      1\n99998  train  [the, movie, was, incredible, ,, ever, since, ...      1\n99999  train  [tcm, came, through, by, acquiring, this, wond...      1\n\n[100000 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>review</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>test</td>\n      <td>[once, again, mr., costner, has, dragged, out,...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>test</td>\n      <td>[this, is, an, example, of, why, the, majority...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>test</td>\n      <td>[first, of, all, i, hate, those, moronic, rapp...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>test</td>\n      <td>[not, even, the, beatles, could, write, songs,...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>test</td>\n      <td>[brass, pictures, (, movies, is, not, a, fitti...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>99995</th>\n      <td>train</td>\n      <td>[delightfully, awful, !, made, by, david, gian...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99996</th>\n      <td>train</td>\n      <td>[watching, time, chasers, ,, it, obvious, that...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99997</th>\n      <td>train</td>\n      <td>[at, the, beginning, we, can, see, members, of...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99998</th>\n      <td>train</td>\n      <td>[the, movie, was, incredible, ,, ever, since, ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99999</th>\n      <td>train</td>\n      <td>[tcm, came, through, by, acquiring, this, wond...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>100000 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# without_stopwords = stopwords_remover(tokenized_reviews)\n# without_stopwords","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:17:24.875074Z","iopub.execute_input":"2025-11-20T15:17:24.875353Z","iopub.status.idle":"2025-11-20T15:17:24.879590Z","shell.execute_reply.started":"2025-11-20T15:17:24.875326Z","shell.execute_reply":"2025-11-20T15:17:24.878690Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"without_punctuation = punctuation_remover(tokenized_reviews)\nwithout_punctuation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:17:24.880861Z","iopub.execute_input":"2025-11-20T15:17:24.881182Z","iopub.status.idle":"2025-11-20T15:17:29.775850Z","shell.execute_reply.started":"2025-11-20T15:17:24.881154Z","shell.execute_reply":"2025-11-20T15:17:29.774862Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"        type                                             review  label\n0       test  [once, again, mr., costner, has, dragged, out,...      0\n1       test  [this, is, an, example, of, why, the, majority...      0\n2       test  [first, of, all, i, hate, those, moronic, rapp...      0\n3       test  [not, even, the, beatles, could, write, songs,...      0\n4       test  [brass, pictures, movies, is, not, a, fitting,...      0\n...      ...                                                ...    ...\n99995  train  [delightfully, awful, made, by, david, giancol...      1\n99996  train  [watching, time, chasers, it, obvious, that, i...      1\n99997  train  [at, the, beginning, we, can, see, members, of...      1\n99998  train  [the, movie, was, incredible, ever, since, i, ...      1\n99999  train  [tcm, came, through, by, acquiring, this, wond...      1\n\n[100000 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>review</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>test</td>\n      <td>[once, again, mr., costner, has, dragged, out,...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>test</td>\n      <td>[this, is, an, example, of, why, the, majority...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>test</td>\n      <td>[first, of, all, i, hate, those, moronic, rapp...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>test</td>\n      <td>[not, even, the, beatles, could, write, songs,...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>test</td>\n      <td>[brass, pictures, movies, is, not, a, fitting,...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>99995</th>\n      <td>train</td>\n      <td>[delightfully, awful, made, by, david, giancol...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99996</th>\n      <td>train</td>\n      <td>[watching, time, chasers, it, obvious, that, i...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99997</th>\n      <td>train</td>\n      <td>[at, the, beginning, we, can, see, members, of...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99998</th>\n      <td>train</td>\n      <td>[the, movie, was, incredible, ever, since, i, ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99999</th>\n      <td>train</td>\n      <td>[tcm, came, through, by, acquiring, this, wond...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>100000 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"tagged_words = pos_tagger(without_punctuation)\ntagged_words","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:17:29.776823Z","iopub.execute_input":"2025-11-20T15:17:29.777101Z","iopub.status.idle":"2025-11-20T15:31:16.848888Z","shell.execute_reply.started":"2025-11-20T15:17:29.777082Z","shell.execute_reply":"2025-11-20T15:31:16.847859Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"        type                                             review  label\n0       test  [(once, ADV), (again, ADV), (mr., ADJ), (costn...      0\n1       test  [(this, DET), (is, VERB), (an, DET), (example,...      0\n2       test  [(first, ADV), (of, ADP), (all, DET), (i, ADJ)...      0\n3       test  [(not, ADV), (even, ADV), (the, DET), (beatles...      0\n4       test  [(brass, NOUN), (pictures, NOUN), (movies, NOU...      0\n...      ...                                                ...    ...\n99995  train  [(delightfully, ADV), (awful, ADJ), (made, VER...      1\n99996  train  [(watching, VERB), (time, NOUN), (chasers, NOU...      1\n99997  train  [(at, ADP), (the, DET), (beginning, NOUN), (we...      1\n99998  train  [(the, DET), (movie, NOUN), (was, VERB), (incr...      1\n99999  train  [(tcm, NOUN), (came, VERB), (through, ADP), (b...      1\n\n[100000 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>review</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>test</td>\n      <td>[(once, ADV), (again, ADV), (mr., ADJ), (costn...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>test</td>\n      <td>[(this, DET), (is, VERB), (an, DET), (example,...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>test</td>\n      <td>[(first, ADV), (of, ADP), (all, DET), (i, ADJ)...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>test</td>\n      <td>[(not, ADV), (even, ADV), (the, DET), (beatles...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>test</td>\n      <td>[(brass, NOUN), (pictures, NOUN), (movies, NOU...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>99995</th>\n      <td>train</td>\n      <td>[(delightfully, ADV), (awful, ADJ), (made, VER...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99996</th>\n      <td>train</td>\n      <td>[(watching, VERB), (time, NOUN), (chasers, NOU...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99997</th>\n      <td>train</td>\n      <td>[(at, ADP), (the, DET), (beginning, NOUN), (we...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99998</th>\n      <td>train</td>\n      <td>[(the, DET), (movie, NOUN), (was, VERB), (incr...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99999</th>\n      <td>train</td>\n      <td>[(tcm, NOUN), (came, VERB), (through, ADP), (b...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>100000 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"lemmatized_words = lemmatizer(tagged_words)\nlemmatized_words","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:31:16.850456Z","iopub.execute_input":"2025-11-20T15:31:16.850804Z","iopub.status.idle":"2025-11-20T15:32:53.241601Z","shell.execute_reply.started":"2025-11-20T15:31:16.850773Z","shell.execute_reply":"2025-11-20T15:32:53.240686Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"        type                                             review  label\n0       test  [once, again, mr., costner, have, drag, out, a...      0\n1       test  [this, be, an, example, of, why, the, majority...      0\n2       test  [first, of, all, i, hate, those, moronic, rapp...      0\n3       test  [not, even, the, beatles, could, write, song, ...      0\n4       test  [brass, picture, movie, be, not, a, fitting, w...      0\n...      ...                                                ...    ...\n99995  train  [delightfully, awful, make, by, david, giancol...      1\n99996  train  [watch, time, chaser, it, obvious, that, it, b...      1\n99997  train  [at, the, beginning, we, can, see, member, of,...      1\n99998  train  [the, movie, be, incredible, ever, since, i, s...      1\n99999  train  [tcm, come, through, by, acquire, this, wonder...      1\n\n[100000 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>review</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>test</td>\n      <td>[once, again, mr., costner, have, drag, out, a...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>test</td>\n      <td>[this, be, an, example, of, why, the, majority...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>test</td>\n      <td>[first, of, all, i, hate, those, moronic, rapp...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>test</td>\n      <td>[not, even, the, beatles, could, write, song, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>test</td>\n      <td>[brass, picture, movie, be, not, a, fitting, w...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>99995</th>\n      <td>train</td>\n      <td>[delightfully, awful, make, by, david, giancol...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99996</th>\n      <td>train</td>\n      <td>[watch, time, chaser, it, obvious, that, it, b...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99997</th>\n      <td>train</td>\n      <td>[at, the, beginning, we, can, see, member, of,...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99998</th>\n      <td>train</td>\n      <td>[the, movie, be, incredible, ever, since, i, s...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99999</th>\n      <td>train</td>\n      <td>[tcm, come, through, by, acquire, this, wonder...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>100000 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"lemmatized_words.to_pickle(\"lemmatized2.pickle\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:32:53.242681Z","iopub.execute_input":"2025-11-20T15:32:53.243436Z","iopub.status.idle":"2025-11-20T15:33:01.748348Z","shell.execute_reply.started":"2025-11-20T15:32:53.243398Z","shell.execute_reply":"2025-11-20T15:33:01.747487Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# lemfile = open('/kaggle/input/lemmatized/lemmatized.pickle', 'rb')\n# lemmatized_words = pickle.load(lemfile)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:33:01.749344Z","iopub.execute_input":"2025-11-20T15:33:01.749596Z","iopub.status.idle":"2025-11-20T15:33:01.753706Z","shell.execute_reply.started":"2025-11-20T15:33:01.749576Z","shell.execute_reply":"2025-11-20T15:33:01.752708Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"lemmatized_words","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:33:01.754646Z","iopub.execute_input":"2025-11-20T15:33:01.754949Z","iopub.status.idle":"2025-11-20T15:33:01.784323Z","shell.execute_reply.started":"2025-11-20T15:33:01.754921Z","shell.execute_reply":"2025-11-20T15:33:01.783464Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"        type                                             review  label\n0       test  [once, again, mr., costner, have, drag, out, a...      0\n1       test  [this, be, an, example, of, why, the, majority...      0\n2       test  [first, of, all, i, hate, those, moronic, rapp...      0\n3       test  [not, even, the, beatles, could, write, song, ...      0\n4       test  [brass, picture, movie, be, not, a, fitting, w...      0\n...      ...                                                ...    ...\n99995  train  [delightfully, awful, make, by, david, giancol...      1\n99996  train  [watch, time, chaser, it, obvious, that, it, b...      1\n99997  train  [at, the, beginning, we, can, see, member, of,...      1\n99998  train  [the, movie, be, incredible, ever, since, i, s...      1\n99999  train  [tcm, come, through, by, acquire, this, wonder...      1\n\n[100000 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>review</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>test</td>\n      <td>[once, again, mr., costner, have, drag, out, a...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>test</td>\n      <td>[this, be, an, example, of, why, the, majority...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>test</td>\n      <td>[first, of, all, i, hate, those, moronic, rapp...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>test</td>\n      <td>[not, even, the, beatles, could, write, song, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>test</td>\n      <td>[brass, picture, movie, be, not, a, fitting, w...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>99995</th>\n      <td>train</td>\n      <td>[delightfully, awful, make, by, david, giancol...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99996</th>\n      <td>train</td>\n      <td>[watch, time, chaser, it, obvious, that, it, b...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99997</th>\n      <td>train</td>\n      <td>[at, the, beginning, we, can, see, member, of,...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99998</th>\n      <td>train</td>\n      <td>[the, movie, be, incredible, ever, since, i, s...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99999</th>\n      <td>train</td>\n      <td>[tcm, come, through, by, acquire, this, wonder...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>100000 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"cp_lemmatized_words = lemmatized_words.copy(deep=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:33:01.785236Z","iopub.execute_input":"2025-11-20T15:33:01.785494Z","iopub.status.idle":"2025-11-20T15:33:01.816309Z","shell.execute_reply.started":"2025-11-20T15:33:01.785467Z","shell.execute_reply":"2025-11-20T15:33:01.815129Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"cp_lemmatized_words[\"review\"] = cp_lemmatized_words.review.map( lambda x: \" \".join(x) )\nreviews = cp_lemmatized_words.review.tolist()\n\nvectorized = CountVectorizer(max_df=0.9, min_df=0.2)\nfitted = vectorized.fit_transform(reviews)\nfitted","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:33:01.817247Z","iopub.execute_input":"2025-11-20T15:33:01.817478Z","iopub.status.idle":"2025-11-20T15:33:18.894132Z","shell.execute_reply.started":"2025-11-20T15:33:01.817460Z","shell.execute_reply":"2025-11-20T15:33:18.893216Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<Compressed Sparse Row sparse matrix of dtype 'int64'\n\twith 3848283 stored elements and shape (100000, 103)>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"X = fitted\ny = cp_lemmatized_words[\"label\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:33:18.895104Z","iopub.execute_input":"2025-11-20T15:33:18.895392Z","iopub.status.idle":"2025-11-20T15:33:18.966086Z","shell.execute_reply.started":"2025-11-20T15:33:18.895369Z","shell.execute_reply":"2025-11-20T15:33:18.965013Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"0.869","metadata":{}},{"cell_type":"code","source":"xgb = XGBClassifier(max_depth=5, n_estimators=1000).fit(X_train, y_train)\n\npred = xgb.predict(X_test)\n\naccuracy_score(y_test, pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:33:18.967063Z","iopub.execute_input":"2025-11-20T15:33:18.967391Z","iopub.status.idle":"2025-11-20T15:33:27.055277Z","shell.execute_reply.started":"2025-11-20T15:33:18.967360Z","shell.execute_reply":"2025-11-20T15:33:27.054560Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"0.7329666666666667"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"0.8699","metadata":{}},{"cell_type":"code","source":"xgb = XGBClassifier(max_depth=3, n_estimators=1000).fit(X_train, y_train)\n\npred = xgb.predict(X_test)\n\naccuracy_score(y_test, pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:33:27.055863Z","iopub.execute_input":"2025-11-20T15:33:27.056080Z","iopub.status.idle":"2025-11-20T15:33:32.015373Z","shell.execute_reply.started":"2025-11-20T15:33:27.056062Z","shell.execute_reply":"2025-11-20T15:33:32.014725Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"0.7416"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"xgb = XGBClassifier(max_depth=3, n_estimators=1000, objective = 'binary:logistic' ).fit(X_train, y_train)\n\npred = xgb.predict(X_test)\n\naccuracy_score(y_test, pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:33:32.017801Z","iopub.execute_input":"2025-11-20T15:33:32.018128Z","iopub.status.idle":"2025-11-20T15:33:36.874154Z","shell.execute_reply.started":"2025-11-20T15:33:32.018105Z","shell.execute_reply":"2025-11-20T15:33:36.872736Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"0.7416"},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"0.851","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\npred_rfc = model.predict(X_test)\naccuracy_score(y_test, pred_rfc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:33:36.876499Z","iopub.execute_input":"2025-11-20T15:33:36.877852Z","iopub.status.idle":"2025-11-20T15:37:58.590107Z","shell.execute_reply.started":"2025-11-20T15:33:36.877822Z","shell.execute_reply":"2025-11-20T15:37:58.589122Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"0.7507333333333334"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"0.8656","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\nmodel = SGDClassifier(max_iter=10000, tol=1e-3)\nmodel.fit(X_train, y_train)\npred_rfc = model.predict(X_test)\naccuracy_score(y_test, pred_rfc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:37:58.591076Z","iopub.execute_input":"2025-11-20T15:37:58.591337Z","iopub.status.idle":"2025-11-20T15:38:01.088513Z","shell.execute_reply.started":"2025-11-20T15:37:58.591318Z","shell.execute_reply":"2025-11-20T15:38:01.087683Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"0.7484666666666666"},"metadata":{}}],"execution_count":26},{"cell_type":"markdown","source":"0.8632","metadata":{}},{"cell_type":"code","source":"model = SGDClassifier(max_iter=10000, loss='hinge', tol=1e-4, alpha=0.0001)\nmodel.fit(X_train, y_train)\npred_rfc = model.predict(X_test)\naccuracy_score(y_test, pred_rfc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:38:01.089317Z","iopub.execute_input":"2025-11-20T15:38:01.089522Z","iopub.status.idle":"2025-11-20T15:38:09.994039Z","shell.execute_reply.started":"2025-11-20T15:38:01.089506Z","shell.execute_reply":"2025-11-20T15:38:09.993139Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"0.75"},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"0.8352","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import BernoulliNB\nmodel = BernoulliNB()\nmodel.fit(X_train, y_train)\npred_rfc = model.predict(X_test)\naccuracy_score(y_test, pred_rfc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:38:09.997914Z","iopub.execute_input":"2025-11-20T15:38:09.998178Z","iopub.status.idle":"2025-11-20T15:38:10.097193Z","shell.execute_reply.started":"2025-11-20T15:38:09.998161Z","shell.execute_reply":"2025-11-20T15:38:10.096083Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"0.7162666666666667"},"metadata":{}}],"execution_count":28},{"cell_type":"markdown","source":"0.8556","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nmodel = MultinomialNB()\nmodel.fit(X_train, y_train)\npred_rfc = model.predict(X_test)\naccuracy_score(y_test, pred_rfc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:38:10.098019Z","iopub.execute_input":"2025-11-20T15:38:10.098271Z","iopub.status.idle":"2025-11-20T15:38:10.140415Z","shell.execute_reply.started":"2025-11-20T15:38:10.098249Z","shell.execute_reply":"2025-11-20T15:38:10.139483Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"0.7036666666666667"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"from sklearn.kernel_approximation import AdditiveChi2Sampler\nfrom sklearn.linear_model import SGDClassifier\n\nchi2sampler = AdditiveChi2Sampler(sample_steps=2)\nX_transformed = chi2sampler.fit_transform(X_train, y_train)\n\nmodel = SGDClassifier(max_iter=50, random_state=0, tol=1e-3)\nmodel.fit(X_transformed, y_train)\npred_rfc = model.predict(chi2sampler.fit_transform(X_test))\naccuracy_score(y_test, pred_rfc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:38:10.141466Z","iopub.execute_input":"2025-11-20T15:38:10.141875Z","iopub.status.idle":"2025-11-20T15:38:12.539109Z","shell.execute_reply.started":"2025-11-20T15:38:10.141845Z","shell.execute_reply":"2025-11-20T15:38:12.538425Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"0.75"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nbagging_classifier = BaggingClassifier(estimator=MultinomialNB(), n_estimators=5)\nbagging_classifier.fit(X_train, y_train)\npredictions = bagging_classifier.predict(X_test)\naccuracy_score(y_test, predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:42:27.746308Z","iopub.execute_input":"2025-11-20T15:42:27.746647Z","iopub.status.idle":"2025-11-20T15:42:28.088307Z","shell.execute_reply.started":"2025-11-20T15:42:27.746607Z","shell.execute_reply":"2025-11-20T15:42:28.086973Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"0.7041333333333334"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nfrom xgboost import XGBClassifier\nbagging_classifier = BaggingClassifier(estimator=XGBClassifier(max_depth=3, n_estimators=1000), n_estimators=5)\nbagging_classifier.fit(X_train, y_train)\npredictions = bagging_classifier.predict(X_test)\naccuracy_score(y_test, predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:49:48.243416Z","iopub.execute_input":"2025-11-20T15:49:48.243788Z","iopub.status.idle":"2025-11-20T15:50:13.409363Z","shell.execute_reply.started":"2025-11-20T15:49:48.243765Z","shell.execute_reply":"2025-11-20T15:50:13.408745Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"0.7433666666666666"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"import pickle\n# Save the trained model as a pickle string.\n\nwith open(\"model.pkl\", \"wb\") as f:\n    pickle.dump(bagging_classifier, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:50:17.372820Z","iopub.execute_input":"2025-11-20T15:50:17.373147Z","iopub.status.idle":"2025-11-20T15:50:17.507585Z","shell.execute_reply.started":"2025-11-20T15:50:17.373121Z","shell.execute_reply":"2025-11-20T15:50:17.506800Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}